{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec52c7b3",
   "metadata": {},
   "source": [
    "Filter Method:\n",
    "The filter method in feature selection involves selecting features based on their statistical properties without involving any machine learning model. It evaluates the relevance of features using statistical measures and selects the most relevant ones.\n",
    "\n",
    "How it Works:\n",
    "Compute Statistical Metrics: For each feature, compute a statistical metric that measures the relationship between the feature and the target variable. Common metrics include correlation coefficients, mutual information, Chi-square scores, and ANOVA F-values.\n",
    "Rank Features: Rank the features based on their scores from the statistical metric.\n",
    "Select Features: Choose the top-ranked features according to a predetermined threshold or a desired number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dbae1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  5]\n",
      " [20  4]\n",
      " [30  3]\n",
      " [40  2]\n",
      " [50  1]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "X = pd.DataFrame({\n",
    "    'feature1': [1, 2, 3, 4, 5],\n",
    "    'feature2': [10, 20, 30, 40, 50],\n",
    "    'feature3': [5, 4, 3, 2, 1]\n",
    "})\n",
    "y = [0, 1, 0, 1, 0]\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "print(X_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba8534",
   "metadata": {},
   "source": [
    "Wrapper Method:\n",
    "The wrapper method involves selecting features based on their performance in a specific machine learning model. It evaluates subsets of features by training and testing a model on them and selecting the subset that yields the best performance.\n",
    "\n",
    "Differences:\n",
    "Model Involvement: The filter method is model-agnostic and purely statistical, while the wrapper method involves training a machine learning model.\n",
    "Evaluation: The filter method uses statistical measures to evaluate features, whereas the wrapper method evaluates the performance of feature subsets using model accuracy, precision, recall, or other relevant metrics.\n",
    "Computational Cost: The wrapper method is typically more computationally intensive than the filter method because it involves training and evaluating multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bc39553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  5]\n",
      " [20  4]\n",
      " [30  3]\n",
      " [40  2]\n",
      " [50  1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = pd.DataFrame({\n",
    "    'feature1': [1, 2, 3, 4, 5],\n",
    "    'feature2': [10, 20, 30, 40, 50],\n",
    "    'feature3': [5, 4, 3, 2, 1]\n",
    "})\n",
    "y = [0, 1, 0, 1, 0]\n",
    "\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, n_features_to_select=2)\n",
    "X_new = rfe.fit_transform(X, y)\n",
    "\n",
    "print(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6038a29a",
   "metadata": {},
   "source": [
    "Embedded Feature Selection Methods:\n",
    "Embedded methods perform feature selection during the model training process and are integrated with specific learning algorithms.\n",
    "\n",
    "Common Techniques:\n",
    "Lasso Regression (L1 Regularization): Introduces a penalty equal to the absolute value of the magnitude of coefficients, effectively shrinking some coefficients to zero.\n",
    "Ridge Regression (L2 Regularization): Adds a penalty equal to the square of the magnitude of coefficients but does not set any coefficients to zero.\n",
    "Tree-Based Methods (e.g., Random Forest, Gradient Boosting): Use feature importance scores based on how much each feature improves the purity of the splits.\n",
    "Elastic Net: Combines L1 and L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5224cb",
   "metadata": {},
   "source": [
    "Drawbacks:\n",
    "Ignores Feature Interactions: The filter method evaluates each feature independently, ignoring potential interactions between features.\n",
    "Model-Agnostic: Since it does not involve a specific machine learning model, it may not select the features that work best for a particular model.\n",
    "Simplicity: The simplicity of the statistical measures used might lead to suboptimal feature selection compared to more complex, model-based methods.\n",
    "Potential for Overlooked Features: Relevant features that do not show strong individual correlation with the target variable might be overlooked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfeabe3",
   "metadata": {},
   "source": [
    "Situations for Using Filter Method:\n",
    "Large Datasets: When dealing with large datasets where computational efficiency is a priority, the filter method is faster and less computationally intensive.\n",
    "Initial Feature Reduction: As a first step to quickly reduce the dimensionality before applying more sophisticated feature selection methods.\n",
    "High-Dimensional Data: In cases of high-dimensional data (many features), where the computational cost of wrapper methods would be prohibitive.\n",
    "Model-Agnostic Requirement: When you need a method that does not depend on a specific machine learning model and provides a general feature selection.\n",
    "Exploratory Data Analysis: For preliminary analysis to understand the relationships between features and the target variable without training multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b07a5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: Index(['size', 'location'], dtype='object')\n",
      "Train MSE: 8.470329472543003e-22\n",
      "Test MSE: 1406250000.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = {\n",
    "    'size': [1500, 1800, 2400, 3000, 3500],\n",
    "    'location': [5, 3, 4, 2, 1],  \n",
    "    'age': [10, 5, 15, 20, 5],\n",
    "    'price': [300000, 350000, 400000, 450000, 500000]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X = df.drop('price', axis=1)\n",
    "y = df['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LinearRegression()\n",
    "selector = RFE(estimator=model, n_features_to_select=2)\n",
    "selector = selector.fit(X_train, y_train)\n",
    "selected_features = X_train.columns[selector.support_]\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "model.fit(X_train[selected_features], y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train[selected_features])\n",
    "y_pred_test = model.predict(X_test[selected_features])\n",
    "\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "print(\"Train MSE:\", train_mse)\n",
    "print(\"Test MSE:\", test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675c8a78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
