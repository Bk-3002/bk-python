{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b78824cd",
   "metadata": {},
   "source": [
    "An ensemble technique in machine learning involves combining multiple models to solve a particular problem and improve the performance of a single model. The idea is that by combining the outputs of multiple models, the ensemble can achieve better predictive performance and robustness than any single model alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0ddabd",
   "metadata": {},
   "source": [
    "Ensemble techniques are used in machine learning to:\n",
    "\n",
    "Improve Accuracy: Combining multiple models typically leads to better predictive performance.\n",
    "Reduce Overfitting: Aggregating models helps in generalizing better to unseen data.\n",
    "Increase Robustness: Ensemble models are less sensitive to the specificities and biases of individual models.\n",
    "Handle Complexity: Complex problems that are difficult for a single model can often be better addressed with a combination of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164bde48",
   "metadata": {},
   "source": [
    "Bagging, or Bootstrap Aggregating, is an ensemble technique where multiple versions of a predictor are trained using different subsets of the training data, obtained by random sampling with replacement (bootstrap samples). The final prediction is typically made by averaging the predictions (for regression) or taking a majority vote (for classification) from all the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eb6f37",
   "metadata": {},
   "source": [
    "Boosting is an ensemble technique that sequentially builds a strong predictor by combining multiple weak learners (models that perform slightly better than random guessing). Each model is trained to correct the errors made by the previous models. The final model is a weighted sum of the predictions of all models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22e43db",
   "metadata": {},
   "source": [
    "The benefits of using ensemble techniques include:\n",
    "\n",
    "Improved Prediction Accuracy: Combining multiple models usually results in better performance.\n",
    "Robustness: Ensembles reduce the likelihood of model-specific errors and biases.\n",
    "Reduced Overfitting: By aggregating predictions, ensembles often generalize better on unseen data.\n",
    "Flexibility: Different types of models can be combined, taking advantage of their strengths.\n",
    "Stability: The variability of predictions due to different training datasets is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4202e373",
   "metadata": {},
   "source": [
    "Ensemble techniques are not always better than individual models. They can be more computationally expensive and complex to implement. If the individual models are already very strong or if the problem is simple, the benefit of using an ensemble might be marginal. Additionally, ensembles might not significantly improve performance if the individual models are highly correlated or if there is insufficient data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24491c41",
   "metadata": {},
   "source": [
    "To calculate the confidence interval using bootstrap:\n",
    "\n",
    "Resample: Generate many bootstrap samples from the original dataset by sampling with replacement.\n",
    "Statistic Calculation: Calculate the statistic of interest (e.g., mean) for each bootstrap sample.\n",
    "Percentile Method: Determine the confidence interval by taking the appropriate percentiles from the bootstrap distribution of the statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d43e9d",
   "metadata": {},
   "source": [
    "Bootstrap is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement. The steps involved in bootstrap are:\n",
    "\n",
    "Sample with Replacement: Randomly sample the original dataset with replacement to create a new dataset of the same size. Repeat this process multiple times to create several bootstrap samples.\n",
    "Calculate Statistic: Compute the statistic of interest (e.g., mean, variance) for each bootstrap sample.\n",
    "Build Distribution: Create a distribution of the computed statistic from all bootstrap samples.\n",
    "Estimate Interval: Use the bootstrap distribution to estimate confidence intervals and other measures of statistical accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d203c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.03384985, 15.06104088])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original sample data\n",
    "np.random.seed(42)\n",
    "sample_heights = np.random.normal(loc=15, scale=2, size=50)\n",
    "\n",
    "# Number of bootstrap samples\n",
    "n_bootstrap_samples = 10000\n",
    "\n",
    "# Generate bootstrap samples and calculate means\n",
    "bootstrap_means = np.array([np.mean(np.random.choice(sample_heights, size=50, replace=True)) for _ in range(n_bootstrap_samples)])\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "confidence_interval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7289aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
