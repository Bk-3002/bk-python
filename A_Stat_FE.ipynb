{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e54b4ab",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "Min-Max Scaling:\n",
    "Min-Max scaling, also known as normalization, transforms the features to a fixed range, usually 0 to 1, but it can be any range, such as -1 to 1. This scaling technique adjusts the values of numeric columns in the dataset so that they fall within a specified range.\n",
    "Example:\n",
    "Consider a dataset with a single feature, price, with values: [10, 20, 30, 40, 50].\n",
    "\n",
    "Find the minimum and maximum values of price: X min =10 and X max=50.\n",
    "Apply the Min-Max scaling formula to each value in the dataset.\n",
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "Unit Vector Scaling:\n",
    "Unit Vector scaling, also known as normalization to unit length, scales the data so that the entire feature vector has a unit norm (i.e., length of 1). This technique is often used in text classification and clustering.\n",
    "\n",
    "Difference from Min-Max Scaling:\n",
    "\n",
    "Min-Max scaling transforms each feature individually based on its minimum and maximum values.\n",
    "Unit Vector scaling transforms the entire feature vector to have a unit norm, preserving the direction but not the magnitude.\n",
    "\n",
    "Q3. What is PCA (Principal Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "Principal Component Analysis (PCA):\n",
    "PCA is a statistical technique used to emphasize variation and bring out strong patterns in a dataset. It does so by transforming the original variables into a new set of variables called principal components, which are orthogonal (uncorrelated) and ordered such that the first few retain most of the variation present in the original variables.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Standardize the data.\n",
    "Calculate the covariance matrix.\n",
    "Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
    "Sort the eigenvalues and their corresponding eigenvectors.\n",
    "Select the top k eigenvectors to form a new feature space (principal components).\n",
    "Example:\n",
    "Consider a dataset with two correlated features: x1 and x2.\n",
    "\n",
    "Standardize the data.\n",
    "Calculate the covariance matrix.\n",
    "Compute eigenvalues and eigenvectors.\n",
    "Project the data onto the top principal components.\n",
    "The result is a lower-dimensional representation of the data, which captures most of the variability with fewer dimensions.\n",
    "\n",
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "Relationship:\n",
    "PCA can be considered a feature extraction method because it transforms the original set of features into a new set of features (principal components) that capture the most important information in the data.\n",
    "\n",
    "Using PCA for Feature Extraction:\n",
    "\n",
    "Compute the principal components.\n",
    "Select the top k principal components that capture the most variance.\n",
    "Project the original data onto these k components to obtain a new feature set.\n",
    "Example:\n",
    "Consider a dataset with features [height, weight, age, blood pressure].\n",
    "\n",
    "Standardize the data.\n",
    "Perform PCA and obtain principal components.\n",
    "Select the top 2 components (if they capture, say, 95% of the variance).\n",
    "Project the data onto these 2 components, reducing the dimensionality while retaining most of the information.\n",
    "\n",
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "Steps to Use PCA:\n",
    "\n",
    "Standardize the data: Ensure all features have a mean of 0 and a standard deviation of 1.\n",
    "Compute the covariance matrix of the standardized data.\n",
    "Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
    "Select the top k eigenvectors corresponding to the largest eigenvalues to form the principal components.\n",
    "Project the original data onto the new feature space defined by these k principal components.\n",
    "Example:\n",
    "\n",
    "Standardize the data:\n",
    "Suppose we have features: revenue, profit, market_trend1, market_trend2, etc.\n",
    "Compute the covariance matrix.\n",
    "Calculate eigenvalues and eigenvectors.\n",
    "Select top k components:\n",
    "Choose k such that the principal components explain, say, 95% of the variance.\n",
    "Transform the data:\n",
    "Project the original data onto these k components.\n",
    "This reduces the number of features while retaining most of the important information, making the model more efficient and potentially more accurate.\n",
    "\n",
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "Steps:\n",
    "\n",
    "Standardize the data (excluding gender as it's categorical and needs to be encoded first).\n",
    "Compute the covariance matrix.\n",
    "Calculate eigenvalues and eigenvectors.\n",
    "Determine the number of principal components to retain by looking at the explained variance ratio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e044f91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'Height': [170, 160, 175, 180],\n",
    "    'Weight': [70, 60, 80, 90],\n",
    "    'Age': [25, 30, 45, 50],\n",
    "    'Gender': ['Male', 'Female', 'Male', 'Female'],\n",
    "    'Blood Pressure': [120, 110, 130, 140]\n",
    "})\n",
    "data_encoded = pd.get_dummies(data, columns=['Gender'])\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_encoded)\n",
    "pca = PCA()\n",
    "pca.fit(data_scaled)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "components_to_retain = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "components_to_retain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e1770",
   "metadata": {},
   "source": [
    "Choosing Principal Components:\n",
    "\n",
    "Calculate the cumulative explained variance.\n",
    "Retain enough components to explain at least 95% of the variance.\n",
    "For example, if the first 3 principal components explain 95% of the variance, retain 3 components.\n",
    "Why:\n",
    "\n",
    "Retaining components that explain 95% of the variance ensures that most of the information in the original features is preserved while reducing dimensionality, which helps in reducing computational complexity and potentially improving model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
